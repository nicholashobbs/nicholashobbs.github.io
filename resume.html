<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Nicholas Hobbs | Extended Resume</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@400;700&display=swap" rel="stylesheet">
</head>
<body>

    <main class="container resume-container">

        <a href="index.html" class="back-link">&larr; Back to Home</a>

        <header class="resume-header">
            <h1>Nicholas Hobbs</h1>
            <div class="contact-info">
                <a href="#">your.email@example.com</a> &bull;
                <a href="#">LinkedIn Profile</a> &bull;
                <a href="#">GitHub Profile</a>
            </div>
        </header>

        <section class="resume-section">
            <h2>Professional Experience</h2>

            <div class="job-entry">
                <h3>Freelance</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>Python / Flask:</strong> Automated inventory reordering and stock level alerts with custom Python script and Flask Dashboard.</li>
                    <li class="resume-item"><strong>SQL:</strong> Designed and optimized SQL queries for analysis of ordering trends, identifying seasonal and cyclical patterns.</li>
                    <li class="resume-item"><strong>Database Manipulation / PostgreSQL / VBA:</strong> Engineered a Visual Basic for Applications (VBA) script to parse and migrate 500+ Word document-based client records into a searchable PostgreSQL database.</li>
                    <li class="resume-item"><strong>Window Functions:</strong> Employed SQL window functions to calculate 30-day rolling average sales, enabling more accurate demand forecasting for appropriate ordering.</li>
                    <li class="resume-item"><strong>CTEs:</strong> Structured complex revenue analysis reports using CTEs to break down sales performance by product category.</li>
                    <li class="resume-item"><strong>NoSQL / MongoDB:</strong> Implemented MongoDB to store and analyze unstructured test subject feedback on UI design, facilitating real-time sentiment analysis.</li>
                    <li class="resume-item"><strong>CSV:</strong> Developed Python scripts to automatically ingest weekly CSV sales reports, streamlining data integration for inventory management.</li>
                    <li class="resume-item"><strong>Airtable:</strong> Designed custom Airtable relational database and forms to input data from salespeople about contacts, interactions, and sales.</li>
                    <li class="resume-item"><strong>Time-series:</strong> Applied ARIMA models to 5 years of sales data, predicting seasonal popularity of specific products and improving predictions.</li>
                    <li class="resume-item"><strong>Forecasting:</strong> Implemented Prophet-based forecasting models for seasonal and holiday specific demand, leading to far better estimations of inventory needs.</li>
                    <li class="resume-item"><strong>AI / Artificial Intelligence:</strong> Deployed an AI-driven sentiment analysis model to categorize 2,000+ customer reviews, identifying key areas for service improvement.</li>
                    <li class="resume-item"><strong>Natural Language Processing / NLP / Fine-tuning:</strong> Built and fine-tuned an NLP model to extract key themes from customer survey responses, easing interpretation of large amounts of textual data.</li>
                    <li class="resume-item"><strong>Computer Vision / CV:</strong> Used OpenCV to analyze satellite data to inform NDVI model, linking with ground based images and improving efficiency for study on land usability in Kenya.</li>
                    <li class="resume-item"><strong>ML Infrastructure / Docker:</strong> Set up a local ML infrastructure using Kubernetes and Docker to streamline model development and support distributed deployment for forecasting and sentiment analysis.</li>
                    <li class="resume-item"><strong>AWS / Amazon Web Services:</strong> Leveraged AWS services (EC2, S3, RDS, EKS) to build and scale data integration platform.</li>
                    <li class="resume-item"><strong>Redshift:</strong> Utilized Amazon Redshift for analytical querying and reporting on aggregated, linked datasets for clients.</li>
                    <li class="resume-item"><strong>Terraform:</strong> Automated the provisioning and management of AWS infrastructure using Terraform, reducing deployment time by 50%.</li>
                    <li class="resume-item"><strong>Model Training:</strong> Orchestrated distributed training of large-scale deep learning models on AWS, completing training cycles 2x faster.</li>
                    <li class="resume-item"><strong>Evaluation Metrics / Evaluate Model Performance:</strong> Monitored forecast accuracy using MAPE and RMSE.</li>
                    <li class="resume-item"><strong>A/B Testing:</strong> Designed and executed A/B tests on separate implementations of UI for ad clickthrough study.</li>
                    <li class="resume-item"><strong>Experimentation:</strong> Conducted a pricing experimentation strategy across product categories, identifying most profitable and price flexible categories.</li>
                    <li class="resume-item"><strong>Docker:</strong> Containerized ad clickthrough study project, ensuring consistent execution across development and production environments.</li>
                    <li class="resume-item"><strong>Linux:</strong> Managed Ubuntu server instances hosting the PostgreSQL database and custom Python applications.</li>
                    <li class="resume-item"><strong>Version Control / Git:</strong> Maintained all project code in Git repositories, facilitating collaborative development and ensuring code integrity for all automation scripts.</li>
                    <li class="resume-item"><strong>Caching Technologies:</strong> Implemented Redis caching for frequently accessed data in A/B testing clickthrough campaign, significantly reducing loading times for accessing relevant data.</li>
                    <li class="resume-item"><strong>Exploratory Data Analysis / EDA / Data Exploration:</strong> Performed extensive EDA on sales data, identifying least profitable categories and worst profitability per time spent on sales.</li>
                    <li class="resume-item"><strong>Tableau:</strong> Developed interactive Tableau dashboards for executive-level revenue and inventory performance tracking, improving reporting clarity.</li>
                    <li class="resume-item"><strong>Test Assumptions / Hypothesis Testing:</strong> Conducted t-tests on A/B test results to statistically validate assumptions about customer behavior impacts on ad clickthrough study.</li>
                    <li class="resume-item"><strong>Designing Experiments / Experimental Design:</strong> Designed a factorial experiment to test the combined impact of pricing and promotional strategies on sales, optimizing campaign ROI.</li>
                    <li class="resume-item"><strong>Simulations:</strong> Ran Monte Carlo simulations to model the impact of different inventory replenishment policies on stock levels and carrying costs.</li>
                    <li class="resume-item"><strong>KPIs / Key Performance Indicators:</strong> Established and tracked KPIs like "average order value" and "inventory turnover rate" to monitor business performance.</li>
                </ul>
            </div>

            <div class="job-entry">
                <h3>Off Piste</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>TypeScript / JavaScript:</strong> Assisted with development of interactive dApp frontend in TypeScript, seamlessly integrating with Solana smart contracts via web3.js.</li>
                    <li class="resume-item"><strong>Node.js:</strong> Built a Node.js backend to serve dynamic data from the Solana blockchain to the D3 visualizations, ensuring real-time updates.</li>
                    <li class="resume-item"><strong>D3:</strong> Created a D3.js-based dashboard visualizing candlestick charts of contract prices, allowing customers to visualize market movements.</li>
                    <li class="resume-item"><strong>Rust:</strong> Designed, implemented, and tested two core Solana smart contracts in Rust, using test driven development, containerization, and breaking ground using new technology.</li>
                    <li class="resume-item"><strong>Async/await / Asynchronous Programming:</strong> Utilized async/await patterns in Rust to manage non-blocking calls to Solana RPC endpoints, improving API responsiveness.</li>
                    <li class="resume-item"><strong>Concurrency:</strong> Engineered concurrent processes in Rust to write asynchronous code that appears sequential, but yields control back to executor when requests are pending, maximizing utilization and throughput.</li>
                    <li class="resume-item"><strong>Parallelization:</strong> Implemented parallel execution for data processing tasks in smart contracts and in Rust testing frameworks, accelerating smart contract test suite by multiples.</li>
                    <li class="resume-item"><strong>Backend / Backend Services / Server-side Development:</strong> Developed scalable Node.js backend services that abstracted blockchain interactions for client-side applications.</li>
                    <li class="resume-item"><strong>Microservices:</strong> Architected the blockchain interaction layer as a set of Node.js microservices, allowing independent scaling and deployment.</li>
                    <li class="resume-item"><strong>Clean Code:</strong> Maintained high standards of test driven development and clean code for all Rust smart contracts and Node.js services, ensuring readability and maintainability.</li>
                    <li class="resume-item"><strong>Unit Tests:</strong> Wrote over 120 unit tests for critical Rust smart contract functions, achieving 90% code coverage and ensuring robust logic in handling transactions.</li>
                    <li class="resume-item"><strong>Test Driven Development / TDD:</strong> Applied TDD principles to Rust smart contract development, containerized testing and production environment for easier collaboration with remote team, and ensured robust development on a new tech stack.</li>
                    <li class="resume-item"><strong>CI/CD / Continuous Integration / Continuous Delivery / Github Actions:</strong> Configured GitHub Actions for CI/CD, automating testing and deployment of smart contracts to devnet.</li>
                    <li class="resume-item"><strong>Linux:</strong> Deployed and managed Node.js backend services and Rust smart contract build environments on containerized Linux virtual machines.</li>
                    <li class="resume-item"><strong>Version Control / Git:</strong> Managed the entire monorepo codebase for smart contracts, backend, and frontend using Git and GitHub.</li>
                    <li class="resume-item"><strong>Distributed System Design:</strong> Designed a resilient distributed system architecture for handling high volumes of blockchain transactions and data queries.</li>
                    <li class="resume-item"><strong>Performance Optimization:</strong> Optimized Rust smart contract execution by profiling and refactoring, achieving a 10% reduction in compute units consumed per transaction.</li>
                    <li class="resume-item"><strong>AWS / Amazon Web Services:</strong> Deployed Node.js backend services and a containerized testing environment on AWS EC2 instances, ensuring scalability.</li>
                    <li class="resume-item"><strong>Full-stack / End-to-end Development:</strong> Managed the complete development cycle from Rust smart contract logic to D3-driven analytics dashboards.</li>
                    <li class="resume-item"><strong>Data Visualization:</strong> Built dynamic D3.js charts to visually represent transaction volume, active users, and token flows on the Solana blockchain.</li>
                </ul>
            </div>
            
            <div class="job-entry">
                <h3>Openlattice</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>Python / Numpy / Matplotlib / Pandas:</strong> Developed Python libraries for data ingestion, cleaning, transformation and visualization using pandas and matplotlib, reducing error and time spent on integrations by an order of magnitude.</li>
                    <li class="resume-item"><strong>D3:</strong> Created custom D3.js interactive network graphs to visualize linked data across law enforcement and local government agencies, improving data discoverability.</li>
                    <li class="resume-item"><strong>SQL / Database Manipulation:</strong> Wrote complex SQL queries to extract, transform, and load data from various government databases into graph structures.</li>
                    <li class="resume-item"><strong>PostgreSQL:</strong> Managed and optimized PostgreSQL instances as the primary data store for integrated agency data on custom graph database.</li>
                    <li class="resume-item"><strong>JSON:</strong> Developed Python parsers for translations between original SQL data, custom graph database, and representation online in data visualizations.</li>
                    <li class="resume-item"><strong>CSV:</strong> Implemented automated CSV ingestion pipeline to standardize and link data from hundreds of disparate government agency spreadsheets.</li>
                    <li class="resume-item"><strong>Java / Kotlin:</strong> Engineered and deployed Java/Kotlin API endpoints for data scientists to query and integrate data, as well as accessing AI model predictions.</li>
                    <li class="resume-item"><strong>Fine-tuning Deep Learning Models / Tensorflow / LLMs / Large Language Models / AI / Artificial Intelligence / Deep Learning:</strong> Fine-tuned a BERT-based model in tensorflow using Hugging Face transformers library with agency-specific text data for improved entity recognition in data linking.</li>
                    <li class="resume-item"><strong>Natural Language Processing / NLP:</strong> Used NLP to extract information from unstructured reports, allowing clients to search through data with specific keywords.</li>
                    <li class="resume-item"><strong>Feature Stores:</strong> Designed and contributed to the implementation of a feature store for managing and serving features for ML-driven deduplication.</li>
                    <li class="resume-item"><strong>Distributed Systems:</strong> Designed and implemented distributed data processing pipelines using Apache Kafka and Apache Spark.</li>
                    <li class="resume-item"><strong>Scalable Data Pipelines / Apache Kafka / Apache Spark:</strong> Built scalable data pipelines using Kafka and Spark, processing real-time data streams from multiple public sector agencies.</li>
                    <li class="resume-item"><strong>Dagster:</strong> Implemented Dagster for orchestrating complex data integration workflows, ensuring repeatability, robustness, and observability across pipelines.</li>
                    <li class="resume-item"><strong>Data Validation:</strong> Implemented automated data validation checks within custom python library, as well as in Dagster pipelines, ensuring data quality before integration into graph databases.</li>
                    <li class="resume-item"><strong>Backend / Backend Services / Server-side Development:</strong> Developed and maintained robust backend endpoints in Kotlin for data access and integration.</li>
                    <li class="resume-item"><strong>REST / RESTful APIs:</strong> Designed and documented RESTful APIs in Kotlin for data scientists and clients to access integrated datasets and AI services.</li>
                    <li class="resume-item"><strong>API Design:</strong> Ensured clean, consistent, and secure API design for critical data access points, facilitating seamless client integrations.</li>
                    <li class="resume-item"><strong>Clean Code:</strong> Championed clean code principles across Kotlin APIs and Python data pipelines.</li>
                    <li class="resume-item"><strong>Docker:</strong> Containerized all data processing jobs, microservices, and AI models for consistent and reproducible deployments.</li>
                    <li class="resume-item"><strong>Version Control / Git:</strong> Managed a large monorepo with Git, implementing branching strategies for collaborative development.</li>
                    <li class="resume-item"><strong>Performance Optimization:</strong> Optimized Spark jobs and PostgreSQL queries, reducing data processing times for critical pipelines.</li>
                    <li class="resume-item"><strong>Exploratory Data Analysis / EDA / Data Exploration:</strong> Performed extensive EDA on raw agency data to identify schema inconsistencies and data quality issues.</li>
                    <li class="resume-item"><strong>Data Visualization:</strong> Built custom D3.js and Matplotlib visualizations for clients, explaining complex data relationships and AI model outputs.</li>
                    <li class="resume-item"><strong>Data Selection:</strong> Collaborated with clients to select optimal datasets for integration, ensuring data relevance and quality for specific use cases.</li>
                    <li class="resume-item"><strong>Test Assumptions / Hypothesis Testing:</strong> Conducted statistical hypothesis testing to validate the effectiveness of various data linking heuristics.</li>
                    <li class="resume-item"><strong>Statistical Inference / Statistical Reasoning:</strong> Applied statistical inference to quantify the confidence in AI-driven data linkages and deduplication results.</li>
                    <li class="resume-item"><strong>QA/QC / Quality Assurance / Quality Control:</strong> Implemented automated QA/QC checks for data pipelines and python libraries, catching 95% of data integrity issues pre-integration.</li>
                    <li class="resume-item"><strong>Graph Methodologies / Graph Analysis:</strong> Built and queried custom graph databases in Kotlin to represent and analyze complex relationships across linked government records.</li>
                    <li class="resume-item"><strong>Data Engineering:</strong> Led data engineering efforts to build robust, scalable, and observable data pipelines for government data integration.</li>
                    <li class="resume-item"><strong>Data Architecture:</strong> Designed the overarching data architecture for a multi-agency data platform, ensuring scalability and data governance.</li>
                    <li class="resume-item"><strong>Data Orchestration (Airflow, Dagster):</strong> Implemented Dagster for orchestrating complex ETL workflows, ensuring timely and reliable data delivery.</li>
                </ul>
            </div>

            <div class="job-entry">
                <h3>Nightfall</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>Python / PyTorch / Large Language Models / LLMs / AI / Deep Learning / NLP:</strong> Built, trained, and fine-tuned state-of-the-art BERT-based LLMs using PyTorch for highly accurate sensitive information detection.</li>
                    <li class="resume-item"><strong>Flask:</strong> Built a Flask web application with a user-friendly interface to accelerate the data labeling process for NLP training data.</li>
                    <li class="resume-item"><strong>Unit Tests:</strong> Wrote comprehensive unit tests for PyTorch model components and Flask API endpoints, ensuring code quality and functionality.</li>
                    <li class="resume-item"><strong>Test Driven Development / TDD:</strong> Applied TDD principles to the development of the Flask labeling application, ensuring robust and reliable features.</li>
                    <li class="resume-item"><strong>Version Control / Git:</strong> Managed the entire codebase for all NLP models, datasets, and the Flask application using Git.</li>
                    <li class="resume-item"><strong>Exploratory Data Analysis / EDA / Data Exploration:</strong> Performed extensive EDA on text datasets to understand linguistic patterns of sensitive information.</li>
                    <li class="resume-item"><strong>Data Visualization:</strong> Created Matplotlib visualizations to analyze model performance (confusion matrices, ROC curves) and data distribution.</li>
                    <li class="resume-item"><strong>Data Selection:</strong> Strategically selected and curated diverse text datasets to train robust and unbiased models for various types of sensitive information.</li>
                    <li class="resume-item"><strong>Challenge Hypotheses:</strong> Challenged the assumption that simple regex rules were sufficient for PII detection by demonstrating the superior accuracy of LLMs when used together with regex.</li>
                    <li class="resume-item"><strong>Designing Experiments / Experimental Design:</strong> Designed rigorous experimental setups to compare the effectiveness and speed of different user interfaces for data tagging experts.</li>
                    <li class="resume-item"><strong>QA/QC / Quality Assurance / Quality Control:</strong> Implemented strict QA/QC protocols for labeled data, ensuring model training data quality and ethical implications of errors.</li>
                    <li class="resume-item"><strong>KPIs / Key Performance Indicators:</strong> Defined and tracked KPIs like "precision," "recall," and "false positive rate" for sensitive data detection models.</li>
                    <li class="resume-item"><strong>Targeting:</strong> Fine-tuned models to specifically target high-risk sensitive information categories, reducing false positives.</li>
                    <li class="resume-item"><strong>AWS / Amazon Web Services:</strong> Leveraged AWS services (EC2, S3, SageMaker) for scalable infrastructure to train and deploy LLMs.</li>
                </ul>
            </div>

            <div class="job-entry">
                <h3>Evolve</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>Anomaly Detection:</strong> Deployed a network anomaly detection system that flagged critical suspicious activities, preventing potential breaches.</li>
                    <li class="resume-item"><strong>Networking Fundamentals:</strong> Leveraged deep knowledge of TCP/IP and network topologies to identify vulnerabilities during penetration tests.</li>
                    <li class="resume-item"><strong>Security Best Practices:</strong> Integrated NIST and OWASP security best practices into all vulnerability assessments.</li>
                    <li class="resume-item"><strong>Search Technologies:</strong> Utilized advanced search technologies including splunk to uncover hidden suspicious traffic and flag misconfigurations across client platforms.</li>
                    <li class="resume-item"><strong>Data Governance:</strong> Conducted comprehensive data governance audits, identifying 5 key areas of non-compliance and providing remediation strategies.</li>
                    <li class="resume-item"><strong>Data Privacy / GDPR / HIPAA:</strong> Performed detailed compliance testing against GDPR and HIPAA regulations, ensuring client data handling procedures met legal standards.</li>
                    <li class="resume-item"><strong>Physical Security / Phishing Awareness:</strong> Performed audit of physical security risks and provided training for employees on avoiding phishing attempts.</li>
                </ul>
            </div>

            <div class="job-entry">
                <h3>Sorenson</h3>
                <ul class="resume-list">
                    <li class="resume-item"><strong>R:</strong> Conducted econometric analysis in R for a Utah state government project, quantifying the ROI of foster care higher education investments at $3-5.00+ for every $1 spent.</li>
                    <li class="resume-item"><strong>Time-series:</strong> Analyzed historical time-series geospatial data of crimes committed in Salt Lake City, identifying key trends in neighborhoods where crime had increased.</li>
                    <li class="resume-item"><strong>Forecasting:</strong> Developed predictive models in R to forecast the long-term impact of foster care interventions on child well-being metrics.</li>
                    <li class="resume-item"><strong>Causal Inference Models:</strong> Applied Propensity Score Matching models and CausalImpact package in R to assess the causal impact of the Justice Reinvestment Initiative on prison recidivism.</li>
                    <li class="resume-item"><strong>Data Validation:</strong> Implemented robust data validation routines in R for large government datasets, ensuring data quality for all analyses.</li>
                </ul>
            </div>

        </section>

    </main>
</body>
</html>