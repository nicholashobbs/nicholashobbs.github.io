<html lang="en">
<head>
  <meta charset="UTF-8">
    <title>Deep Learning</title>

    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/dreampulse/computer-modern-web-font/master/fonts.css">
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
  <style type = "text/css">
  #container {display:table;width:100%}
  body {
  font-family: 'Computer Modern Serif', serif;
}
</style>
</head>

<body>
<h1> Deep Learning - Goodfellow, Bengio, Courville </h1>

<h2> Ch 1 - Intro </h2>
<p>The <strong>knowledge base</strong> approach to AI seeks to hard-code knowledge about the world into a formal language.</p>
<p><strong>Logistic regression</strong> - refers to regression with a categorical outcome (dependent) variable</p>
<p>Machine learning depends on <strong>representation</strong> - the way that a set of informative variables known as <strong>features</strong> is organized.</p>
<p><strong>Representation learning</strong> uses the machine to discover representation itself, instead of just mapping representation to output.</p>
<p>An <strong>autoencoder</strong> is a combination of an <strong>encoder</strong> which converts input data to representation, and a <strong>decoder</strong>, which converts the new representation back to its original format.</p>
<p>One of the keys to designing features is to find <strong>factors of variation</strong>, which are descriptive measures to make sense of rich variability in the data.</p>
<p>Deep learning allows representation learning to work by giving the computer a set of simpler representations out of which to build more complex concepts.</p>
<p>A <strong>feedforward deep network</strong> or <strong>multilayer perceptron</strong> is a mathematical function mapping input values to output values through complex representations composed of many simpler functions.</p>
<p>Each layer of representation can be thought of as the state of the computer's memory after executing another set of instructions. Networks with greater depth can execute more instructions in sequence.</p>
<p>The input is presented at the <strong>visible layer</strong>, while <strong>hidden layers</strong> extract increasingly abstract features and analyze them.</p>
<p>Deep learning has been historically known as cybernetics and then connectionism.</p>
<p>The <strong>mccullough pitts neuron</strong> and <strong>adaptive linear element (ADALINE)</strong> are early models of brain function. <strong>Stochastic gradient descent</strong> is a training algorithm which allows a model to learn the correct weights for its factors.</p>
<p>Linear models can not learn the XOR function.</p>
<p>A <strong>neocognitron</strong> is a model architecture for processing images inspired by the mammalian visual system.</p>
<p>Most neural networks today use a <strong>rectified linear unit</strong>.</p>
<p><strong>Distributed representation</strong> refers to the concept that each input to a system should be represented by many features, and that each feature should be involved in the representation of many inputs.</p>

<h2> Ch 2 - Linear Algebra </h2>
<p>A <strong>scalar</strong> is a single number, such as $s \in \mathbb{R}$ or $n \in \mathbb{N}$</p>
<p>A <strong>vector</strong> is an array of numbers. Each member of an array has an index, and the ith element of a vector $\textbf{x}$ is $x_{i}$</p>
<p>A <strong>matrix</strong> is a 2d array of numbers. If a matrix $\textbf{A}$ has m rows and n columns, $\textbf{A} \in \mathbb{R}^{m \times n}$ and the element in the ith row and jth column is $\textbf{A}_{i,j}$. $\textbf{A}_{i,:}$ denotes the ith row, and $\textbf{A}_{:,j}$ denotes the jth column</p>
<p>A <strong>tensor</strong> is an array with more than two axes. The element of a tensor $\textbf{A}$ at coordinates $(i,j,k)$ is denoted $\textbf{A}_{i,j,k}$</p>
<p> The <strong>transpose</strong> of a matrix is simply a reflection along the <strong>main diagonal</strong> - elements for which $i=j$. The transpose of matrix $A$ is $A^{T}$, where each element $(A^{T})_{i,j} = A_{j,i}$. Since a vector is a matrix with one column, the transpose of a vector is a matrix with one row.</p>
<p>If two matrices have the same shape, they can be added together by adding their corresponding elements. For $\textbf{C}=\textbf{A}+\textbf{B}$, $\textbf{C}_{i,j}=\textbf{A}_{i,j}+\textbf{B}_{i,j}$. We can add a scalar to a matrix or multiply a matrix by a scalar simply by performing the operation on each element of the matrix. $\textbf{D}=a \cdot \textbf{B}+c$</p>
<p>In deep learning, we can use a technique called <strong>broadcasting</strong> to add a vector $\textbf{b}$ to each row of the matrix whereby $\textbf{C}=\textbf{A}+\textbf{b}$ gives $\textbf{C}_{i,j}=\textbf{A}_{i,j}+\textbf{b}_{j \cdot}$</p>
<p>The <strong>matrix product</strong> of matrices $\textbf{A}$ and $\textbf{B}$ exists if $\textbf{A}$ has the same number of columns as $\textbf{B}$ has rows. The result is a third matrix $\textbf{C}=\textbf{AB}$ which has as many rows as $\textbf{A}$ and as many columns as $\textbf{B}$. The operation is defined $\textbf{C}_{i,j}=\sum_{k}\textbf{A}_{i,k}\textbf{B}_{k,j}$</p>
<p>The <strong>hadamard product</strong> or <strong>element-wise product</strong>, denoted $\textbf{A} \odot \textbf{B}$ gives a matrix containing the product of the individual elements.</p>
<p>The <strong>dot product</strong> of two vectors $\textbf{x}$ and $\textbf{y}$ is the matrix product $\textbf{x}^{T}\textbf{y}$. The matrix product $\textbf{C}=\textbf{A}\textbf{B}$ computes $\textbf{C}_{i,j}$ as the dot product between row $i$ of $\textbf{A}$ and column $j$ of $\textbf{B}$</p>
<p>The matrix product is distributive $\textbf{A}(\textbf{B}+\textbf{C})=\textbf{A}\textbf{B}+\textbf{A}\textbf{C}$, associative $\textbf{A}(\textbf{B}\textbf{C})=(\textbf{A}\textbf{B})\textbf{C}$, but not commutative $\textbf{A}\textbf{B} \ne \textbf{B}\textbf{A}$</p>
<p>The dot product between two vectors is commutative $\textbf{x}^{T}\textbf{y}=\textbf{y}^{T}\textbf{x}$</p>
<p>The transpose of a matrix product $(\textbf{A}\textbf{B})^{T}=\textbf{B}^{T}\textbf{A}^{T}$</p>
<p>A system of linear equations $\textbf{Ax}=\textbf{b}$ has a known matrix $\textbf{A} \in \mathbb{R}^{m \times n}$ and a known vector $\textbf{b} \in \mathbb{R}^{m}$. $\textbf{x} \in \mathbb{R}^{n}$ is a vector of unknown variables we would like to solve for.</p>
<p>The <strong>matrix inverse</strong> of $\textbf{A}$, denoted $\textbf{A}^{-1}$ allows us to solve systems of linear equations using the formula $\textbf{A}^{-1}\textbf{A}=\textbf{I}_{n}$, which gives us the formula to find $\textbf{x}=\textbf{A}^{-1}\textbf{b}$. For $\textbf{A}^{-1}$ to exist, $\textbf{Ax}=\textbf{b}$ must have exactly one solution for each $\textbf{b}$</p>
<p>The <strong>origin</strong> refers to a vector of all zeroes. The number of ways to travel from the origin to $\textbf{b}$, where $\textbf{x}_{i}$ determines how far to move in the direction of column i.</p>
<p>A <strong>linear combination</strong> of some set of vectors $\textbf{v}^{(1)} ... \textbf{v}^{(n)}$ is given by $\sum_{i}c_{i}\textbf{v}^{(i)}$</p>
<p>The <strong>span</strong> of a set of vectors is the set including all points which can be reached by linear combination of the $\textbf{v}^{(i)}$. Therefore, finding whether $\textbf{Ax}=\textbf{b}$ has a solution is equivalent to testing whether $\textbf{b}$ is in the span of the columns of $\textbf{A}$. This span is known as the <strong>column space</strong>or the <strong>range</strong>of $\textbf{A}$.</p>
<p>For the system $\textbf{Ax}=\textbf{b}$ to have a solution for all values $\textbf{b} \in \mathbb{R}^{m}$, the column space of $\textbf{A}$ must include all of $\mathbb{R}^{m}$, implying that $\textbf{A}$ must have at least $m$ columns. Since columns can have redundant information, this by itself is not sufficient.</p>
<p>If two columns are redundant, they are refered to as <strong>linearly dependent</strong>. A set of vectors is <strong>linearly independent</strong> if no vector in the set is a linear combination of other vectors.</p>
<p>A matrix is <strong>square</strong> if all the columns are linearly independent and the number of rows is the same as the number of columns. A square matrix with linearly dependent columns is <strong>singular</strong>.</p>
<p>A <strong>norm</strong> is a function used to measure the size of vectors. The $L^{p}$ norm is given by $\left\|x\right\|_{p}=(\sum_{i}|x_{i}|^{p})^{1/p}$ for $p \in \mathbb{R}, p \ge 1$. The $L^{2}$ norm with $p=2$ is known as the <strong>Euclidean norm</strong>. The Euclidean norm is used very frequently in machine learning, and often denoted as $\left\|x\right\|$. It is common to calculate the size of vectors using the squared $L^{2}$ norm, which can be calculated as $\textbf{x}^{T}\textbf{x}$. Another norm commonly used in machine learning is the $L^{1}$ norm $\left\|x\right\|_{1}=\sum_{i}|x_{i}|$, which is good for discriminating between elements that are exactly zero and those which are small but nonzero.</p>
<p>The $L^{\inf}$ norm, known as the <strong>max norm</strong> is given by $\left\|x\right\|_{\inf}=\max_{i}|x_{i}|$.</p>
<p>In machine learning, the obscure <strong>frobenius norm</strong> $\left\|A\right\|_{F}=\sqrt{\sum_{i,j}A_{i,j}^{2}}$ is a common way to measure the size of a matrix.</p>
<p>A <strong>diagonal matrix</strong> consists mostly of zeroes, with nonzero entries along the main diagonal. $\textbf{D}$ is diagonal $\iff D_{i,j}=0$ for all $i \ne j$. diag($\textbf{v}$) denotes a square diagonal matrix whose diagonal entries are the elements of the vector $\textbf{v}$. Diagonal matrices have many properties which make them easy to manipulate. For example, diag$(\textbf{v})\textbf{x} = \textbf{v} \odot \textbf{x}$, and if every diagonal entry is nonzero, diag$(\textbf{v})^{-1} =$ diag$([1/v_{1},...,1/v_{n}]^{T})$</p>
<p>A <strong>unit vector</strong> is a vector with <strong>unit norm</strong> $\left\|x\right\|_{2}=1$</p>
<p>A <strong>symmetric</strong> matrix is any for which $\textbf{A}=\textbf{A}^{T}$</p>
<p>Vectors $\textbf{x}$ and $\textbf{y}$ are <strong>orthogonal</strong> if $\textbf{x}^{T}\textbf{y}=0$. If vectors are orthogonal and have a unit norm, they are <strong>orthonormal</strong>. An <strong>orthogonal matrix</strong> $\textbf{A}^{T}\textbf{A}=\textbf{A}\textbf{A}^{T}=\textbf{I}$, implying that $\textbf{A}^{-1}=\textbf{A}^{T}$</p>
<p><strong>Eigendecomposition</strong> refers to the process of decomposing a matrix into a set of <strong>eigenvectors</strong> and <strong>eigenvalues</strong>. An eigenvector of a square matrix $\textbf{A}$ is a nonzero vector $\textbf{v}$ such that $\textbf{Av}=\lambda\textbf{v}$, where the scalar $\lambda$ is the eigenvalue corresponding to this eigenvector. If we have a matrix $\textbf{A}$ with $n$ linearly independent eigenvectors with corresponding eigenvalues, we concatenate the eigenvectors to form a matrix $\textbf{V}$ with one eigenvector per column. We also concatenate the eigenvalues to form a vector $\lambda$. The eigendecomposition of $\textbf{A}$ is then given by $\textbf{A}=\textbf{V}$diag$(\lambda)\textbf{V}^{-1}$. If we have an orthogonal matrix $\textbf{Q}$ composed of eigenvectors of $\textbf{A}$ and a diagonal matrix $\Lambda$ where each $\Lambda_{i,i}$ is associated with the eigenvector $\textbf{Q}_{:,i}$, every real symmetric matrix can be decomposed into $\textbf{A}=\textbf{Q}\Lambda\textbf{Q}^{T}$. The entries of $\Lambda$ are normally sorted in descending order. A matrix is singular $\iff$ any of the eigenvalues are zero. </p>
<p>A <strong>positive definite</strong> matrix is one with only positive eigenvalues. A <strong>positive semidefinite</strong> matrix is one whose eigenvalues are all positive or 0. A <strong>negative semidefinite</strong> matrix is one whose eigenvalues are negative or 0, and a <strong>negative definite</strong> matrix is one with only negative eigenvalues.</p>
<p><strong>Singular value decomposition</strong> is another way to factorize a matrix into <strong>singular vectors</strong> and <strong>singular values</strong>. Every matrix, including those which are not square, has an SVD. For $\textbf{A}$ $m times n$, $\textbf{U}$ orthogonal and $m times m$ with columns known as <strong>left-singular vectors</strong>, $\textbf{D}$ diagonal and $m times n$ whose elements are the singular values of $\textbf{A}$, $\textbf{V}$ orthogonal and $n times n$ with columns known as <strong>right-singular vectors</strong>, $\textbf{A}=\textbf{UDV}^{T}$. The left-singular vectors of $\textbf{A}$ are the eigenvectors of $\textbf{A}\textbf{A}^{T}$, while the right-singular vectors of $\textbf{A}$ are eigenvectors of $\textbf{A}^{T}\textbf{A}$.</p>
<p>The <strong>Moore-Penrose pseudoinverse</strong> is defined as a matrix $\textbf{A}^{+}=\lim _{ \alpha \searrow 0 }{(\textbf{A}^{T}\textbf{A}+\alpha\textbf{I})^{-1}\textbf{A}^{T}}=\textbf{VD}^{+}\textbf{U}^{T}$, where $\textbf{U}$,$\textbf{D}$, and $\textbf{V}$ are the singular value decomposition of $\textbf{A}$, and $\textbf{D}^{+}$ is found by taking the reciprocal of the elements of a diagonal matrix $\textbf{D}$ and then taking the transpose of this matrix.</p>
<p>The <strong>trace</strong> of a matrix $\textbf{A}$ is the sum of its diagonal entries Tr$(\textbf{A})=\sum_{i}\textbf{A}_{i,i}$. The frobenius norm can be rewritten $\left\|A\right\|_{F}=$Tr$\sqrt{\textbf{A}\textbf{A}^{T}}$</p>
<p>The <strong>determinant</strong> of a square matrix det$(\textbf{A})$ is the product of its eigenvalues. The absolute value of the determinant is a measure of how much multiplication by the matrix expands or contracts space. Multiplication by a matrix with a determinant of 0 contracts space completely along at least one dimension, meaning the resulting matrix has no volume. If the determinant is 1, the transformation preserves volume.</p>
<p><strong>Principal components analysis</strong> is a simple machine learning algorithm that involves an decoder function $\textbf{D}$ and corresponding encoder $\textbf{D}^{T}$. Generally, the matrix $\textbf{D}$ is given by the $l$ eigenvectors corresponding to the largest eigenvalues.</p>

<h2> Ch 3 - Probability </h2>
<p><strong>inherent stochastity</strong></p>
<p><strong>incomplete observability</strong></p>
<p><strong>incomplete modeling</strong></p>
<p><strong>degrees of belief</strong></p>
<p><strong>frequentist/bayesian</strong></p>
<p><strong>random variable</strong></p>
<p><strong>probability distributions</strong></p>
<p><strong>pmf</strong></p>
<p><strong>jpdf</strong></p>
<p><strong>pdf</strong></p>
<p><strong>discrete/continuous</strong></p>
<p><strong>uniform distribution</strong></p>
<p><strong>marginal probability</strong></p>
<p><strong>conditional probability</strong></p>
<p><strong>intervention query</strong></p>
<p><strong>causal modeling</strong></p>
<p><strong>chain rule of probability</strong></p>
<p><strong>independence</strong></p>
<p><strong>conditional independence</strong></p>
<p><strong>expectation</strong></p>
<p><strong>variance</strong></p>
<p><strong>standard deviation</strong></p>
<p><strong>correlation</strong></p>
<p><strong>covariance matrix</strong></p>
<p><strong>bernoulli distribution</strong></p>
<p><strong>multinomial</strong></p>
<p><strong>gaussian</strong></p>
<p><strong>central limit theorem</strong></p>
<p><strong>multivariate normal distribution</strong></p>
<p><strong>precision matrix</strong></p>
<p><strong>isotropic</strong></p>
<p><strong>exponential distribution</strong></p>
<p><strong>laplace distribution</strong></p>
<p><strong>dirac delta function</strong></p>
<p><strong>empirical distribution</strong></p>
<p><strong>mixture distribution</strong></p>
<p><strong>latent variable</strong></p>
<p><strong>gaussian mixture</strong></p>
<p><strong>logistic sigmoid</strong></p>
<p><strong>saturates</strong></p>
<p><strong>softplus function</strong></p>
<p><strong>positive part function</strong></p>
<p><strong>bayes rule</strong></p>
<p><strong>measure theory</strong></p>
<p><strong>measure zero</strong></p>
<p><strong>almost everywhere</strong></p>
<p><strong>jacobian matrix</strong></p>
<p><strong>information theory</strong></p>
<p><strong>nats</strong></p>
<p><strong>bits</strong></p>
<p><strong>shannons</strong></p>
<p><strong>self information</strong></p>
<p><strong>differential entroy</strong></p>
<p><strong>kullback-leiber divergence</strong></p>
<p><strong>cross-entropy</strong></p>
<p><strong>graphical model</strong></p>
<p><strong>directed model</strong></p>
<p><strong>proportionality</strong></p>

<h2> Ch 4 - Numerical Computation </h2>
<p><strong>underflow</strong></p>
<p><strong>overflow</strong></p>
<p><strong>multinoulli</strong></p>
<p><strong>condition number</strong></p>
<p><strong>objective function</strong></p>
<p><strong>criterion</strong></p>
<p><strong>cost function</strong></p>
<p><strong>loss function</strong></p>
<p><strong>error function</strong></p>
<p><strong>derivative</strong></p>
<p><strong>gradient descent</strong></p>
<p><strong>critical points</strong></p>
<p><strong>local min/max</strong></p>
<p><strong>saddle point </strong></p>
<p><strong>global min/max</strong></p>
<p><strong>directional derivative</strong></p>
<p><strong>learning rate</strong></p>
<p><strong>live search</strong></p>
<p><strong>hill climbing</strong></p>
<p><strong>curvature/2nd derivative</strong></p>
<p><strong>hessian matrix</strong></p>
<p><strong>first order optimization</strong></p>
<p><strong>second order</strong></p>
<p><strong>lipschitz continuous</strong></p>
<p><strong>lipschitz constant</strong></p>
<p><strong>convex optimization</strong></p>
<p><strong>constrained optimization</strong></p>
<p><strong>feasible</strong></p>
<p><strong>karush kuhn-tucker</strong></p>
<p><strong>generalized lagrangian</strong></p>
<p><strong>equality/inequality contraints</strong></p>
<p><strong>active contstraint</strong></p>
<p><strong>linear least squares</strong></p>

<h2> Ch 5 - ML Basics </h2>
<p><strong>learning</strong></p>
<p><strong>example</strong></p>
<p><strong>features</strong></p>
<p><strong>classification</strong></p>
<p><strong>missing inputs</strong></p>
<p><strong>regression</strong></p>
<p><strong>transcription</strong></p>
<p><strong>translation</strong></p>
<p><strong>structured output</strong></p>
<p><strong>anomaly detection</strong></p>
<p><strong>synthesis/sampling</strong></p>
<p><strong>imputation of missing values</strong></p>
<p><strong>denoising</strong></p>
<p><strong>density estimation</strong></p>
<p><strong>accuracy</strong></p>
<p><strong>error rate</strong></p>
<p><strong>test set</strong></p>
<p><strong>unsupervised</strong></p>
<p><strong>supervised</strong></p>
<p><strong>dataset</strong></p>
<p><strong>data points</strong></p>
<p><strong>reinforcement learning</strong></p>
<p><strong>design matrix</strong></p>
<p><strong>linear regression</strong></p>
<p><strong>parameters</strong></p>
<p><strong>weights</strong></p>
<p><strong>mean squared error</strong></p>
<p><strong>normal equations</strong></p>
<p><strong>bias</strong></p>
<p><strong>generalization error</strong></p>
<p><strong>training error</strong></p>
<p><strong>statistical learning theory</strong></p>
<p><strong>iid assumptions</strong></p>
<p><strong>data generating distribution</strong></p>
<p><strong>under/overfitting</strong></p>
<p><strong>capacity</strong></p>
<p><strong>hypothesis space</strong></p>
<p><strong>input</strong></p>
<p><strong>parameters</strong></p>
<p><strong>representational capacity</strong></p>
<p><strong>effective capacity</strong></p>
<p><strong>occams razor</strong></p>
<p><strong>vapnik-chernovekis dimension</strong></p>
<p><strong>non-parametric models</strong></p>
<p><strong>nearest neighbor</strong></p>
<p><strong>under/over fitting regimes</strong></p>
<p><strong>opimal capacity</strong></p>
<p><strong>bayes error</strong></p>
<p><strong>no free lunch theorem</strong></p>
<p><strong>weight decay</strong></p>
<p><strong>regulizer</strong></p>
<p><strong>regularization</strong></p>
<p><strong>hyper parameters</strong></p>
<p><strong>validation set</strong></p>
<p><strong>cross-validation</strong></p>
<p><strong>estimates</strong></p>
<p><strong>bias</strong></p>
<p><strong>variance</strong></p>
<p><strong>point estimator</strong></p>
<p><strong>statistic</strong></p>
<p><strong>unbiased</strong></p>
<p><strong>asymptotically unbiased</strong></p>
<p><strong>standard error</strong></p>
<p><strong>consistency</strong></p>
<p><strong>almost sure convergence</strong></p>
<p><strong>maximum likelihood</strong></p>
<p><strong>log-likelihood</strong></p>
<p><strong>conditional</strong></p>
<p><strong>statistic efficiency</strong></p>
<p><strong>parametric case</strong></p>
<p><strong>CRLB</strong></p>
<p><strong>prior probability distribution</strong></p>
<p><strong>a priori</strong></p>
<p><strong>posterior distribution</strong></p>
<p><strong>MAP estimation</strong></p>
<p><strong>supervised</strong></p>
<p><strong>support vector machines</strong></p>
<p><strong>kernel trick</strong></p>
<p><strong>gaussian kernel</strong></p>
<p><strong>template matching</strong></p>
<p><strong>kernel methods</strong></p>
<p><strong>support vectors</strong></p>
<p><strong>simpler representations</strong></p>
<p><strong>k-means cluster</strong></p>
<p><strong>SGD</strong></p>
<p><strong>curse of dimensionality</strong></p>
<p><strong>local constancy prior</strong></p>
<p><strong>local kernels</strong></p>
<p><strong>composition of factors</strong></p>
<p><strong>manifold learning</strong></p>
<p><strong>manifold hypothesis</strong></p>

<h2> Ch 6 - Deep Feedforward Networks </h2>
<p><strong>deep feedforward network</strong></p>
<p><strong>multilayer perceptron</strong></p>
<p><strong>feedback</strong></p>
<p><strong>recurrent neural networks</strong></p>
<p><strong>output layer</strong></p>
<p><strong>hidden layers</strong></p>
<p><strong>width</strong></p>
<p><strong>phi nonlinear transformation</strong></p>
<p><strong>activation functions</strong></p>
<p><strong>back-propagation</strong></p>
<p><strong>rectified linear unit</strong></p>
<p><strong>cost function</strong></p>
<p><strong>calculus of variations</strong></p>
<p><strong>mean absolute error</strong></p>
<p><strong>sigmoid units</strong></p>
<p><strong>softmax units</strong></p>
<p><strong>winner take all</strong></p>
<p><strong>heteroskedastic</strong></p>
<p><strong>misture density networks</strong></p>
<p><strong>clip gradients</strong></p>
<p><strong>hidden units</strong></p>
<p><strong>rectified linear units</strong></p>
<p><strong>absolute value rectification</strong></p>
<p><strong>maxout units</strong></p>
<p><strong>leaky/parametric ReLU</strong></p>
<p><strong>catastrophic forgetting</strong></p>
<p><strong>radial basis function</strong></p>
<p><strong>softplus</strong></p>
<p><strong>hard/hidden units</strong></p>
<p><strong>architecture</strong></p>
<p><strong>universal approximation theorem</strong></p>
<p><strong>forward/back propagation</strong></p>
<p><strong>computational graph</strong></p>
<p><strong>recursive chain rule is backprop</strong></p>
<p><strong>symbolic representations</strong></p>
<p><strong>numeric value</strong></p>
<p><strong>tensor V</strong></p>
<p><strong>dynamic programming</strong></p>
<p><strong>automatic differentiation</strong></p>
<p><strong>reverse mode accumulation</strong></p>
<p><strong>forward mode accumulation</strong></p>
<p><strong>hessian</strong></p>
<p><strong>krylov methods</strong></p>
<p><strong>sparse activations</strong></p>

<h2> Ch 7 - Regularization in Deep Learning </h2>
<p><strong>parameter norm penalties</strong></p>
<p><strong>L2 parameter regularization</strong></p>
<p><strong>weight decay</strong></p>
<p><strong>tikhonov regularization</strong></p>
<p><strong>L1 regularization</strong></p>
<p><strong>ridge regression</strong></p>
<p><strong>feature selection</strong></p>
<p><strong>karush kuhn-tucker multiplier</strong></p>
<p><strong>under constrained</strong></p>
<p><strong>dropout</strong></p>
<p><strong>augmentation</strong></p>
<p><strong>noise robustness</strong></p>
<p><strong>label smoothing</strong></p>
<p><strong>semi-supervised</strong></p>
<p><strong>multi-task</strong></p>
<p><strong>early stopping</strong></p>
<p><strong>parameter sharing</strong></p>
<p><strong>convolutional neural nets</strong></p>
<p><strong>orthogonal</strong></p>
<p><strong>matching pursuit</strong></p>
<p><strong>bagging (bootstrap aggregating)</strong></p>
<p><strong>model averaging</strong></p>
<p><strong>ensemble methods</strong></p>
<p><strong>boosting</strong></p>
<p><strong>weight scaling inference rule</strong></p>
<p><strong>dropout boosting</strong></p>
<p><strong>fast dropout</strong></p>
<p><strong>adversarial example</strong></p>
<p><strong>adversarial training</strong></p>
<p><strong>virtual adversarial example</strong></p>
<p><strong>tangent distance algorithm</strong></p>
<p><strong>tangent prop algorithm</strong></p>
<p><strong>double back prop</strong></p>
<p><strong>manifold tangent classifiers</strong></p>

<h2> Ch 8 - Optimization for Deep Learning </h2>
<p><strong>theta parameters</strong></p>
<p><strong>j of theta cost function</strong></p>
<p><strong>data generating distribution</strong></p>
<p><strong>risk</strong></p>
<p><strong>empirical risk</strong></p>
<p><strong>surragate loss function</strong></p>
<p><strong>batch/deterministc</strong></p>
<p><strong>stochastic/online</strong></p>
<p><strong>minibatch</strong></p>
<p><strong>minibatch stochastic</strong></p>
<p><strong>generalization error</strong></p>
<p><strong>stream</strong></p>
<p><strong>model identifiability</strong></p>
<p><strong>weight space symmetry</strong></p>
<p><strong>saddle free newton method</strong></p>
<p><strong>gradient clipping heuristic</strong></p>
<p><strong>cliffs</strong></p>
<p><strong>explaining gradients</strong></p>
<p><strong>vanishing gradients</strong></p>
<p><strong>power method</strong></p>
<p><strong>excess error</strong></p>
<p><strong>momentum</strong></p>
<p><strong>nesterov momentum</strong></p>
<p><strong>correction factor</strong></p>
<p><strong>normalized initialization</strong></p>
<p><strong>sparse initialization</strong></p>
<p><strong>adagrad</strong></p>
<p><strong>rmsprop</strong></p>
<p><strong>adam</strong></p>
<p><strong>conjugate directions</strong></p>
<p><strong>fletcher-reeves</strong></p>
<p><strong>polak-ribune</strong></p>
<p><strong>nonlinear conjugate dradients</strong></p>
<p><strong>bfgs algorithm</strong></p>
<p><strong>limitied memory bfgs</strong></p>
<p><strong>batch normalization</strong></p>
<p><strong>coordinate descent</strong></p>
<p><strong>polyak averaging</strong></p>
<p><strong>supervised pretraining</strong></p>
<p><strong>fine tuning</strong></p>
<p><strong>greedy</strong></p>
<p><strong>continuation methods</strong></p>
<p><strong>curriculum learning</strong></p>

<h2> Ch 9 - Convolutional Neural Nets </h2>
<p><strong>pooling</strong></p>
<p><strong>convolution</strong></p>
<p><strong>feature map</strong></p>
<p><strong>cross-correlation</strong></p>
<p><strong>toeplitz matrix</strong></p>
<p><strong>dobsky block circulant matrix</strong></p>
<p><strong>parameter sharing</strong></p>
<p><strong>equivariant representations</strong></p>
<p><strong>tied weights</strong></p>
<p><strong>equivariance</strong></p>
<p><strong>invariant</strong></p>
<p><strong>permutation invariant</strong></p>
<p><strong>stride</strong></p>
<p><strong>unshared convolution</strong></p>
<p><strong>tiled convolution</strong></p>
<p><strong>structured outputs</strong></p>
<p><strong>separable</strong></p>
<p><strong>primary visual center</strong></p>
<p><strong>simple/complex cells</strong></p>
<p><strong>fovea</strong></p>
<p><strong>saccades</strong></p>
<p><strong>time delay neural nets</strong></p>
<p><strong>reverse correlation</strong></p>
<p><strong>gabor functions</strong></p>
<p><strong>quadrature pair</strong></p>

<h2> Ch 10 - Recurrent Neural Nets </h2>
<p><strong>unfolding back propagation through time</strong></p>
<p><strong>teacher forcing</strong></p>
<p><strong>open-loop</strong></p>
<p><strong>stationary</strong></p>
<p><strong>optimizing</strong></p>
<p><strong>encoder</strong></p>
<p><strong>reader</strong></p>
<p><strong>input</strong></p>
<p><strong>decoder</strong></p>
<p><strong>writer</strong></p>
<p><strong>output</strong></p>
<p><strong>attention mechanisms</strong></p>
<p><strong>echo state networks</strong></p>
<p><strong>liquid state machines</strong></p>
<p><strong>reservoir computing</strong></p>
<p><strong>spectral radius</strong></p>
<p><strong>contractive</strong></p>
<p><strong>leaky units</strong></p>
<p><strong>gated RNNs</strong></p>
<p><strong>long short term memory</strong></p>
<p><strong>gated recurrent unit</strong></p>
<p><strong>forget-gate unit</strong></p>
<p><strong>external input gate</strong></p>
<p><strong>output gate</strong></p>
<p><strong>clipping the norm</strong></p>
<p><strong>explicit memory</strong></p>
<p><strong>working memory</strong></p>
<p><strong>memory networks</strong></p>
<p><strong>neural turing machine</strong></p>
<p><strong>content based addressing</strong></p>
<p><strong>location based addressing</strong></p>
<p><strong>attention mechanism</strong></p>

<h2> Ch 11 - Practical Methodologies </h2>
<p><strong>precision</strong></p>
<p><strong>recall</strong></p>
<p><strong>f-score</strong></p>
<p><strong>pr-curve integral</strong></p>
<p><strong>converge</strong></p>
<p><strong>hyperparameter optimization</strong></p>
<p><strong>grid search</strong></p>
<p><strong>finite differences</strong></p>
<p><strong>centered difference</strong></p>

<h2> Ch 12 - Applications </h2>
<p><strong>coalesced</strong></p>
<p><strong>gp-gpus</strong></p>
<p><strong>data parallelism</strong></p>
<p><strong>model parallelism</strong></p>
<p><strong>asynchronous stochastic gradient descent</strong></p>
<p><strong>parameter server</strong></p>
<p><strong>model compression</strong></p>
<p><strong>dynamic structure</strong></p>
<p><strong>conditional computation</strong></p>
<p><strong>cascade</strong></p>
<p><strong>gater</strong></p>
<p><strong>expert networks</strong></p>
<p><strong>mixture of experts</strong></p>
<p><strong>hard mixture</strong></p>
<p><strong>global contrast normalization</strong></p>
<p><strong>sphering</strong></p>
<p><strong>whitening</strong></p>
<p><strong>local contrast normalization</strong></p>
<p><strong>automatic speech recognition</strong></p>
<p><strong>natural language processing</strong></p>
<p><strong>n-grams</strong></p>
<p><strong>language model smoothing</strong></p>
<p><strong>back-off methods</strong></p>
<p><strong>neural language models</strong></p>
<p><strong>word embeddings</strong></p>
<p><strong>shortlist</strong></p>
<p><strong>hierarchical softmax</strong></p>
<p><strong>postive/negative phase</strong></p>
<p><strong>importance sampling</strong></p>
<p><strong>biased importance sampling</strong></p>
<p><strong>maximum entropy language models</strong></p>
<p><strong>read</strong></p>
<p><strong>memory</strong></p>
<p><strong>exploit</strong></p>
<p><strong>collaborative filtering</strong></p>
<p><strong>content based recommender systems</strong></p>
<p><strong>contextual bandits</strong></p>
<p><strong>relation</strong></p>
<p><strong>binary relation</strong></p>
<p><strong>relational databases</strong></p>
<p><strong>attributes</strong></p>
<p><strong>knowledge base</strong></p>
<p><strong>link prediction</strong></p>
<p><strong>word-sense disambiguation</strong></p>

<h2> Ch 13 - Linear Factor Models </h2>
<p><strong>factor analysis</strong></p>
<p><strong>conditionally independent</strong></p>
<p><strong>capture dependencies</strong></p>
<p><strong>probabilistic PCA</strong></p>
<p><strong>reconstruction error</strong></p>
<p><strong>nonlinear independent components estimation</strong></p>
<p><strong>independent subspace analysis</strong></p>
<p><strong>topographic ICA</strong></p>
<p><strong>slow feature analysis</strong></p>
<p><strong>sparse coding</strong></p>

<h2> Ch 14 - Autoencoders </h2>
<p><strong>recirculation</strong></p>
<p><strong>undercomplete</strong></p>
<p><strong>overcomplete</strong></p>
<p><strong>sparse</strong></p>
<p><strong>actual zeroes</strong></p>
<p><strong>denoising autoencoder</strong></p>
<p><strong>contractive autoencoder</strong></p>
<p><strong>encoding function</strong></p>
<p><strong>encoding distribution</strong></p>
<p><strong>reconstruction distribution</strong></p>
<p><strong>denoising score matching</strong></p>
<p><strong>tangent planes</strong></p>
<p><strong>nearest neighbor graph</strong></p>
<p><strong>contractive autoencoders</strong></p>
<p><strong>predictive sparse decomposition</strong></p>
<p><strong>learned approximate inference</strong></p>
<p><strong>information retrieval</strong></p>
<p><strong>semantic hashing</strong></p>

<h2> Ch 15 - Representation Learning </h2>
<p><strong>unsupervised pretraining</strong></p>
<p><strong>greedy algorithm</strong></p>
<p><strong>fine-tuning</strong></p>
<p><strong>transfer learning</strong></p>
<p><strong>domain adaptation</strong></p>
<p><strong>concept drift</strong></p>
<p><strong>one-shot learning</strong></p>
<p><strong>zero-data learning</strong></p>
<p><strong>multi-modal learning</strong></p>
<p><strong>generative adversarial networks</strong></p>
<p><strong>symbolic representation</strong></p>
<p><strong>sum product network</strong></p>

<h2> Ch 16 - Structured Probabilistic Models </h2>
<p><strong>probabilistic models</strong></p>
<p><strong>graphical models</strong></p>
<p><strong>directed graphical model</strong></p>
<p><strong>structured bayesian network</strong></p>
<p><strong>local conditional probability distributions</strong></p>
<p><strong>undirected models</strong></p>
<p><strong>markov random fields</strong></p>
<p><strong>clique potential</strong></p>
<p><strong>partition function</strong></p>
<p><strong>energy based model</strong></p>
<p><strong>boltzman distribution/machines</strong></p>
<p><strong>product of experts</strong></p>
<p><strong>free energy</strong></p>
<p><strong>separation</strong></p>
<p><strong>context-specific independencies</strong></p>
<p><strong>immorality</strong></p>
<p><strong>moralized graphs</strong></p>
<p><strong>chordal/triangulated</strong></p>
<p><strong>factor graphs</strong></p>
<p><strong>ancestral sampling</strong></p>
<p><strong>gibbs sampling</strong></p>
<p><strong>loopy belief propagation</strong></p>
<p><strong>harmonium</strong></p>
<p><strong>restricted boltzman machine</strong></p>

<h2> Ch 17 - Monte Carlo Methods </h2>
<p><strong>LLN</strong></p>
<p><strong>approximation by average</strong></p>
<p><strong>CLT</strong></p>
<p><strong>biased importance sampling</strong></p>
<p><strong>markov chain monte carlo</strong></p>
<p><strong>ancestral sampling</strong></p>
<p><strong>stationary/equilibrium distribution</strong></p>
<p><strong>stochastic matrices</strong></p>
<p><strong>burning in</strong></p>
<p><strong>mixing time</strong></p>
<p><strong>block gibbs sampling</strong></p>
<p><strong>tempered transitions</strong></p>
<p><strong>critical temperations</strong></p>
<p><strong>parralel tempering</strong></p>

<h2> Ch 18 - Partition Function </h2>
<p><strong>partition is integral or summation</strong></p>
<p><strong>positive/negative phase</strong></p>
<p><strong>hallucinations</strong></p>
<p><strong>fantasy particles</strong></p>
<p><strong>contrastive divergence</strong></p>
<p><strong>spurious modes</strong></p>
<p><strong>stochastic maximum likelihood</strong></p>
<p><strong>persistent contrastive divergence</strong></p>
<p><strong>fast PCD</strong></p>
<p><strong>pseudolikelihood noise contrastive estimation</strong></p>
<p><strong> noise distribution</strong></p>
<p><strong>self contrastive estimation</strong></p>
<p><strong>bridge the gap</strong></p>
<p><strong>annealed importance sampling</strong></p>
<p><strong>bridge sampling</strong></p>
<p><strong>linked importance sampling</strong></p>

<h2> Ch 19 - Approximate Inference </h2>
<p><strong>evidence lower bound</strong></p>
<p><strong>variational</strong></p>
<p><strong>maximum a posteriori inference</strong></p>
<p><strong>mean field approach</strong></p>
<p><strong>structured variational inference</strong></p>
<p><strong>binary sparse coding</strong></p>
<p><strong>damping</strong></p>
<p><strong>calculus of variations</strong></p>
<p><strong>euler-lagrange equation</strong></p>

<h2> Ch 20 - Deep Generative Models </h2>
<p><strong>boltzmann machines</strong></p>
<p><strong>harmonium</strong></p>
<p><strong>deep belief networks</strong></p>
<p><strong>deep boltzmann machine</strong></p>
<p><strong>multi-prediction deep boltzmann machine</strong></p>
<p><strong>centered deep boltzmann</strong></p>
<p><strong>enhanced gradient</strong></p>
<p><strong>spike/slab restricted boltzmann</strong></p>
<p><strong>convolutional boltzmann</strong></p>
<p><strong>probablistic max pooling</strong></p>
<p><strong>reparameterization trick</strong></p>
<p><strong>stochastic back-propagation</strong></p>
<p><strong>perturbation analysis</strong></p>
<p><strong>REINFORCE algorithm</strong></p>
<p><strong>baseline</strong></p>
<p><strong>variance reduction</strong></p>
<p><strong>variance normalization</strong></p>
<p><strong>sigmoid belief networks</strong></p>
<p><strong>generator network</strong></p>
<p><strong>inverse transform sampling</strong></p>
<p><strong>variational autoencoder</strong></p>
<p><strong>importance weighted autoencoder</strong></p>
<p><strong>deep reccurent attention writer</strong></p>
<p><strong>discriminator network</strong></p>
<p><strong>self-supervised boosting</strong></p>
<p><strong>generative moment matching networks</strong></p>
<p><strong>maximum mean discrepancy</strong></p>
<p><strong>fully visible bayes networks</strong></p>
<p><strong>reuse of features</strong></p>
<p><strong>parameters</strong></p>
<p><strong>neural autoregressive density estimator</strong></p>
<p><strong>generalized denoising autoencoders</strong></p>
<p><strong>clamping</strong></p>
<p><strong>generative stochastic networks</strong></p>
<p><strong>detailed balance</strong></p>
<p><strong>walk-back training</strong></p>
<p><strong>diffusion inversion</strong></p>
<p><strong>approximate bayesian computation</strong></p>

</body>
